{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "import jieba.analyse\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "# import matplotlib.cm as cm\n",
    "# from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# 数据清洗：加载数据、添加自定义词典、分词、去停用词和标点等\n",
    "data = [jieba.lcut(str(i).strip('\\n')) for i in open('data/muchong_questions.txt', 'r', encoding='utf-8').readlines()]\n",
    "cn_stopwords = [line.strip('\\n') for line in open('stopwords/hit_stopwords.txt', 'r', encoding='utf-8').readlines()]\n",
    "for idx, sentence in enumerate(data):\n",
    "    data[idx] = ' '.join([word for word in sentence if word not in cn_stopwords])\n",
    "# TODO:user_dict\n",
    "# data[0:5]  # 看一下数据长啥样"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "(12861, 2)"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''基于TF-IDF的文本向量化表示'''\n",
    "vectorizer = TfidfVectorizer(min_df=3, max_features=20000, encoding='latin-1')\n",
    "X = vectorizer.fit_transform(data)\n",
    "# 文本向量降维\n",
    "svd = TruncatedSVD(2)  #设置降维后的维度\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "X = lsa.fit_transform(X)\n",
    "# 看一下处理后数据的维度\n",
    "X.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:18<00:00,  2.34s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": "   num_clusters  avg_silhouette_score\n0             2              0.676559\n1             3              0.645244\n2             4              0.675485\n3             5              0.639797\n4             6              0.609622\n5             7              0.582649\n6             8              0.554621\n7             9              0.569523",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>num_clusters</th>\n      <th>avg_silhouette_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>0.676559</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>0.645244</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>0.675485</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5</td>\n      <td>0.639797</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6</td>\n      <td>0.609622</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>7</td>\n      <td>0.582649</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>8</td>\n      <td>0.554621</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>9</td>\n      <td>0.569523</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用轮廓系数绘制学习曲线找出最优类别数，轮廓系数越大越好\n",
    "class_silscore = []\n",
    "for n_clusters in trange(2, 10):  # 设置可接受的类别数量范围，如2类到9类\n",
    "    # fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    # fig.set_size_inches(18, 7)\n",
    "    # ax1.set_xlim([-0.1, 1])\n",
    "    # ax1.set_ylim([0, X.shape[0] + (n_clusters + 1) * 10])\n",
    "    clusters = KMeans(n_clusters=n_clusters, init='k-means++').fit(X)\n",
    "    cluster_labels = clusters.labels_\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    class_silscore.append([n_clusters, silhouette_avg])\n",
    "    # sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "df_class_silscore = pd.DataFrame(class_silscore, columns=['num_clusters', 'avg_silhouette_score'])\n",
    "df_class_silscore"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "'''使用最优类别数进行聚类'''\n",
    "max_silscore_idx = df_class_silscore['avg_silhouette_score'].idxmax()  # 获得最大轮廓系数对应的最优类簇数\n",
    "best_k = df_class_silscore['num_clusters'].iloc[max_silscore_idx]\n",
    "num_cluster = best_k  # 填入上一步输出的最大平均轮廓系数对应的类别数\n",
    "best_clusters = KMeans(n_clusters=num_cluster, random_state=10).fit(X)  # 获得最优聚类结果\n",
    "labels = best_clusters.labels_.tolist()  #获得每条原始文本聚类后归入的类别标签\n",
    "df_clusters = pd.concat([pd.DataFrame(labels, columns=['cluster_id']), pd.DataFrame(data, columns=['text'])], axis=1)\n",
    "df_clusters.to_excel('TF-IDF-clustered_data.xlsx')  # 输出每个文本及其聚类后归入的类别"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "'''获得每个类簇的包含的原始文本'''\n",
    "cluster_text = {}\n",
    "for i in range(num_cluster):\n",
    "    cluster_text[i] = []\n",
    "    for j in np.where(best_clusters.labels_ == i)[0].tolist():\n",
    "        cluster_text[i] += data[j]  # TODO:完善提取关键词的流程，如去停用词、 同义词归一化等。\n",
    "    cluster_text[i] = ''.join(cluster_text[i])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "'''输出每个类簇中基于TF-IDF的关键词'''\n",
    "# vectorizer = CountVectorizer(token_pattern=r'(?u)\\b\\w+\\b')\n",
    "# transformer = TfidfTransformer() # norm=None, smooth_idf=False\n",
    "# x = transformer.fit_transform(vectorizer.fit_transform(cluster_text[i] for i in range(num_cluster)))\n",
    "# print(x)\n",
    "for i in range(num_cluster):\n",
    "    keywords = jieba.analyse.extract_tags(cluster_text[i], topK=100, allowPOS=())\n",
    "    with open('keywords_cluster_{}.txt'.format(i), 'w', encoding='utf-8') as f:\n",
    "        for j in keywords:\n",
    "            f.write(j + '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}